---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Hongxiang Li is a final year master's student at Peking University, supervised by [Yuexian Zou](https://scholar.google.com/citations?user=sfyr7zMAAAAJ&hl=zh-CN).




My research interests include Visual Generation and Video Understanding.

If interested in collaboration or discussion, please [email](lihxxxxxx@gmail.com
) me.



# üî• News
- *2025.02*: &nbsp; One paper is accepted to IEEE TCSVT. 
- *2025.01*: &nbsp; One paper is accepted to ICLR 2025. 
- *2024.12*: &nbsp; One paper is accepted to IEEE TMI.
- *2024.08*: &nbsp; Two papers are accepted to ECCV 2024.
- *2024.07*: &nbsp; One paper is accepted to ACM MM 2024.
- *2023.12*: &nbsp; One paper is accepted to AAAI 2024.
- *2023.07*: &nbsp; Two papers are accepted to ICCV 2023. G2L is accepted as oral.

# üìù Selected Publications
See full list at [Google Scholar](https://scholar.google.com/citations?view_op=list_works&hl=zh-CN&authuser=1&hl=zh-CN&user=U4AwycUAAAAJ&authuser=1).
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR2025</div><img src='images/dispose.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[DisPose: Disentangling Pose Guidance for Controllable Human Image Animation](https://arxiv.org/abs/2412.09349)

**Hongxiang Li**, Yaowei Li, Yuhang Yang, Junjie Cao, Zhihong Zhu, Xuxin Cheng, Long Chen<sup>#</sup> <br>

<a href="https://lihxxx.github.io/DisPose/">Project</a>, <a href="https://arxiv.org/abs/2412.09349">Paper</a>, <a href="https://github.com/lihxxx/DisPose">Code</a> <br>

International Conference on Learning Representations (ICLR), 2025
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI2024</div><img src='images/aaai2024-acnet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[Exploiting Auxiliary Caption for Video Grounding](https://ojs.aaai.org/index.php/AAAI/article/view/29812)

**Hongxiang Li**, Meng Cao, Xuxin Cheng, Yaowei Li, Zhihong Zhu, Yuexian Zou<sup>#</sup> <br>

<a href="https://ojs.aaai.org/index.php/AAAI/article/view/29812">Paper</a> <br>

AAAI Conference on Artificial Intelligence (AAAI), 2024
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV2023 Oral</div><img src='images/iccv2023-g2l.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[G2L: Semantically Aligned and Uniform Video Grounding via Geodesic and Game Theory](https://openaccess.thecvf.com/content/ICCV2023/html/Li_G2L_Semantically_Aligned_and_Uniform_Video_Grounding_via_Geodesic_and_ICCV_2023_paper.html)

**Hongxiang Li**, Meng Cao, Xuxin Cheng, Yaowei Li, Zhihong Zhu, Yuexian Zou<sup>#</sup> <br>

<a href="https://openaccess.thecvf.com/content/ICCV2023/html/Li_G2L_Semantically_Aligned_and_Uniform_Video_Grounding_via_Geodesic_and_ICCV_2023_paper.html">Paper</a> <br>

International Conference on Computer Vision (ICCV), 2023 (Oral)
</div>
</div>

<!-- [**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div> -->

# üíª Internships
- *2023 - 2024* Baidu VIS, Shenzhen, China.
 <!-- Worked with [Bi Li](https://scholar.google.com/citations?user=DI0gfQ8AAAAJ&hl=zh-CN&oi=ao).-->
- *2023 - 2023* Tencent YouTu Lab, Shanghai, China.
<!--  Worked with [Bangjie Yin](https://scholar.google.com/citations?user=HDEAoJAAAAAJ&hl=zh-CN&oi=ao) and [Shen Chen](https://chenshen.xyz/).-->
- *2021 - 2022* SenseTime Research, Chengdu, China.
<!--  Worked with [Yongqiang Yao](https://scholar.google.com/citations?hl=zh-CN&user=DztTSf0AAAAJ&view_op=list_works&sortby=pubdate).-->

# üéñ Honors and Awards
- *2023* Merit Student Pacesetter, Peking University. 
- *2021* National Scholarship. 

<!-- # üìñ Educations
- *2019.06 - 2022.04 (now)*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2015.09 - 2019.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->

<!-- # Academic Services
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/) -->

<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=312&t=tt&d=6QsWf4KK-03cp2_GNpoY-H9i_4ZaVbHSNRup-os9Ras'></script>
